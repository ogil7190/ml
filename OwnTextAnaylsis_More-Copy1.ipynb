{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    @ogil7190:\\n     - We read files from a folder (assuming located in same directory as this is in)\\n         - file structure \\n             text_data:\\n                 /: athism\\n                 /: electronics\\n                 .\\n                 .\\n     - We split files into test and train \\n     - we calculate word list based on train_data\\n         - during training we move to each file, first clean headers ( logic for header removing is that Word 'Lines:' exists in almost every file, we remove everything on till that line)\\n         - get word list for that file using list_word_from_file()\\n         - compare against stop words \\n         - store words\\n    - we need to calculate prior probabilites also, for that, we need :: word, how many times encountered in a folder,  how many total words in from that folder and count of files scanned in that folder\\n    - use laplace correction for smoothing\\n    - calculate probabilities for each word given a class\\n    - use predict() to predict on a file\\n    - use predict_bulk()  to predict test_data we prepared\\n    - use print_word_list() helper to print word_list to see how data is coming.\\n    \\n    structure of word list :\\n    \\n    { word : { class : frequencies, ... , prob : { class : probabililty}}\\n    each word contain frequencies against each class, and probability p (word | class) :: p of word given a class\\n    \\n    prior list keep count of files scanned and total words of each class\\n\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    @ogil7190:\n",
    "     - We read files from a folder (assuming located in same directory as this is in)\n",
    "         - file structure \n",
    "             text_data:\n",
    "                 /: athism\n",
    "                 /: electronics\n",
    "                 .\n",
    "                 .\n",
    "     - We split files into test and train \n",
    "     - we calculate word list based on train_data\n",
    "         - during training we move to each file, first clean headers ( logic for header removing is that Word 'Lines:' exists in almost every file, we remove everything on till that line)\n",
    "         - get word list for that file using list_word_from_file()\n",
    "         - compare against stop words \n",
    "         - store words\n",
    "    - we need to calculate prior probabilites also, for that, we need :: word, how many times encountered in a folder,  how many total words in from that folder and count of files scanned in that folder\n",
    "    - use laplace correction for smoothing\n",
    "    - calculate probabilities for each word given a class\n",
    "    - use predict() to predict on a file\n",
    "    - use predict_bulk()  to predict test_data we prepared\n",
    "    - use print_word_list() helper to print word_list to see how data is coming.\n",
    "    \n",
    "    structure of word list :\n",
    "    \n",
    "    { word : { class : frequencies, ... , prob : { class : probabililty}}\n",
    "    each word contain frequencies against each class, and probability p (word | class) :: p of word given a class\n",
    "    \n",
    "    prior list keep count of files scanned and total words of each class\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/20_newsgroup/\" #relative path of folder newsgroup\n",
    "LIMIT = -1 # how many files to read from one folder, -1 is for all\n",
    "ALPHA = 0.001 # Laplace correction alpha for smoothing\n",
    "TEST_SIZE_PERCENT = 0.1 # how much to scale test data size 20%, 30 % etc.\n",
    "PRUNE_TOP_WORD_COUNT = 25 # Remove top keys as these keys will remove variance among data while taking porbabilites\n",
    "stop_words = [\"article\", \"writes\", \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\",\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"dont\", \"don't\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\"this\", \"those\", \"though\", \"through\", \"throughout\",\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\"under\", \"until\", \"up\", \"upon\", \"us\",\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "#stop_words.append(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '<', '>', '', 'article', 'writes']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    files = list()\n",
    "    file_list = os.listdir(os.getcwd()+path)\n",
    "    for filename in file_list:\n",
    "        files.append(filename)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    return ''.join( c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_words(files, FOLDER_LOC):\n",
    "    my_word_list = {}\n",
    "    prior = {}\n",
    "    count = 0\n",
    "    files_count = 0\n",
    "    for f in files.keys():\n",
    "        try:\n",
    "            prior[f]\n",
    "        except:\n",
    "            prior[f] = {}\n",
    "        for k in files[f]:\n",
    "            files_count += 1\n",
    "            try:\n",
    "                prior[f]['files_count'] += 1\n",
    "            except:\n",
    "                prior[f]['files_count'] = 1\n",
    "            file_path = FOLDER_LOC + f + \"/\" + k\n",
    "            my_words = list_word_from_file(file_path)\n",
    "            for w in my_words:\n",
    "                try:\n",
    "                    prior[f]['words_count'] += 1\n",
    "                except:\n",
    "                    prior[f]['words_count'] = 1\n",
    "                try:\n",
    "                    val = my_word_list[w] \n",
    "                    val['total_count'] += 1\n",
    "                    try:\n",
    "                        val[f] += 1\n",
    "                    except:\n",
    "                        val[f] = 1\n",
    "                except:\n",
    "                    my_word_list[w] = {'total_count' : len(files.keys()) + 1 }\n",
    "                    for x in files.keys():\n",
    "                        my_word_list[w][x] = ALPHA\n",
    "                    my_word_list[w][f] += 1\n",
    "        count += 1\n",
    "        print('Processed:', count, 'Folders out of:', len(files.keys()))\n",
    "    prior['total_count'] = files_count\n",
    "    return my_word_list, prior\n",
    "\n",
    "\n",
    "def list_word_from_file(file_path):\n",
    "    my_words = list()\n",
    "    with open(file_path[1:], 'r', errors='ignore') as doc:\n",
    "                pos = False\n",
    "                for line in doc:\n",
    "                    if pos:\n",
    "                        break_line = line.lower().split()\n",
    "                        for x in break_line:\n",
    "                            w = clean(x) #clean a word, sometimes contain other symbols\n",
    "                            if not re.search(\"\\d\", w) and len(w) > 2: # word contain any number, then drop it\n",
    "                                if w not in stop_words: # if it is a stop word, drop it\n",
    "                                    my_words.append(w)\n",
    "                    try:\n",
    "                        line.index('Lines:') # found end of header in email\n",
    "                        pos = True\n",
    "                    except:\n",
    "                        print('',end='')\n",
    "    return my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(my_word_list, name):\n",
    "    with open(name, 'wb') as fp:\n",
    "        pickle.dump(my_word_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load(name):\n",
    "    with open(name, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word_list(word_list, limit = -1):\n",
    "    for k in list(word_list.keys())[0:limit]:\n",
    "        print(k, word_list[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHFData(my_word_list):\n",
    "    for i in range(PRUNE_TOP_WORD_COUNT):\n",
    "        my_word_list = prune(my_word_list)\n",
    "    return my_word_list\n",
    "\n",
    "def prune(my_words):\n",
    "    max_key = '-1'\n",
    "    max_len = 0\n",
    "    for i in my_words:\n",
    "        if max_len < my_words[i]['total_count']:\n",
    "            max_len = my_words[i]['total_count']\n",
    "            max_key = i\n",
    "    del my_words[max_key]\n",
    "    return my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SIZE, files, shuffle = True):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    for k in files:\n",
    "        file_list = files[k]\n",
    "        if shuffle:\n",
    "            random.shuffle(file_list)\n",
    "        test_size = math.ceil(SIZE * len(file_list))\n",
    "        train_size = len(file_list) - test_size\n",
    "        train[k] = file_list[0: train_size]\n",
    "        test[k] = file_list[train_size : train_size + test_size]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(FOLDER_LOC, FILES_LIMIT = 10):\n",
    "    folders = get_files(FOLDER_LOC)\n",
    "    files = {}\n",
    "    for i in folders:\n",
    "        if not i.startswith('.'): # not a file\n",
    "            files[i] = get_files(FOLDER_LOC + i)[0:FILES_LIMIT]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(files, FOLDER_LOC):\n",
    "    start_time = time.time()\n",
    "    words = list_words(files, FOLDER_LOC)\n",
    "    finish_time = time.time()\n",
    "    print('Time Taken:', finish_time - start_time, 'seconds')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_prob(words_list, prior): #calculate prior probability for each word against each class\n",
    "    for w in words_list:\n",
    "        for k in list(words_list[w].keys()):\n",
    "            if k == 'total_count':\n",
    "                continue\n",
    "            prob = words_list[w][k] / ( prior[k]['words_count'] + len(words_list))\n",
    "            try:\n",
    "                words_list[w]['prob'][k] = prob\n",
    "            except:\n",
    "                words_list[w]['prob'] = { k : prob }\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, prior, file, FILE_LOC):\n",
    "    words = list_word_from_file(FILE_LOC + '/' + file)\n",
    "    prob = dict()\n",
    "    for w in words:\n",
    "        try:\n",
    "            word_data = data[w]\n",
    "            for k in word_data:\n",
    "                if k == 'total_count':\n",
    "                    continue\n",
    "                try:\n",
    "                    prob[k] += np.log(word_data['prob'][k])\n",
    "                except:\n",
    "                    prob[k] = np.log(prior[k]['files_count'] / (ALPHA * prior['total_count'])) + np.log(word_data['prob'][k])\n",
    "        except:\n",
    "            print('', end = '')\n",
    "    return prob\n",
    "\n",
    "def predict_bulk(my_test_data, words_list, print_log = True): # return accuracy only\n",
    "    acc = 0\n",
    "    tot = 0\n",
    "    for k in my_test_data:\n",
    "        tot += 1\n",
    "        count = 0\n",
    "        for i in range(len(my_test_data[k])):\n",
    "            max_val = - float(\"inf\")\n",
    "            max_i = ''\n",
    "            ans = predict(words_list, prior, my_test_data[k][i], DATA_FOLDER + k)\n",
    "            for x in ans:\n",
    "                if max_val < ans[x]:\n",
    "                    max_val = ans[x]\n",
    "                    max_i = x\n",
    "            if k == max_i:\n",
    "                count += 1\n",
    "        if print_log:\n",
    "            print('Correctly Predicted:', count, 'Total:', len(my_test_data[k]), 'Class:', k)\n",
    "        acc += count / len(my_test_data[k])\n",
    "    acc = (acc / tot) * 100\n",
    "    if print_log:\n",
    "        print('Accuracy:', acc,'%')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 1 Folders out of: 20\n",
      "Processed: 2 Folders out of: 20\n",
      "Processed: 3 Folders out of: 20\n",
      "Processed: 4 Folders out of: 20\n",
      "Processed: 5 Folders out of: 20\n",
      "Processed: 6 Folders out of: 20\n",
      "Processed: 7 Folders out of: 20\n",
      "Processed: 8 Folders out of: 20\n",
      "Processed: 9 Folders out of: 20\n",
      "Processed: 10 Folders out of: 20\n",
      "Processed: 11 Folders out of: 20\n",
      "Processed: 12 Folders out of: 20\n",
      "Processed: 13 Folders out of: 20\n",
      "Processed: 14 Folders out of: 20\n",
      "Processed: 15 Folders out of: 20\n",
      "Processed: 16 Folders out of: 20\n",
      "Processed: 17 Folders out of: 20\n",
      "Processed: 18 Folders out of: 20\n",
      "Processed: 19 Folders out of: 20\n",
      "Processed: 20 Folders out of: 20\n",
      "Time Taken: 15.47261905670166 seconds\n"
     ]
    }
   ],
   "source": [
    "my_files_list = read_data(DATA_FOLDER, LIMIT) #list of files against folder, type : dictionary\n",
    "train_data, test_data = split_data(TEST_SIZE_PERCENT, my_files_list) # split data into train and testing, type : dictionary\n",
    "my_words_list, prior = fit(train_data, DATA_FOLDER) # fit on training data, type : dictionary\n",
    "my_words_list = populate_prob(my_words_list, prior) # calculate prior probabilites, type : dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_words_list = removeHFData(my_words_list) # use it to remove highly frequent data from list mostly un-useful words\n",
    "save(my_words_list, 'MyWordList.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Predicted: 9 Total: 10 Class: talk.politics.mideast\n",
      "Correctly Predicted: 5 Total: 10 Class: rec.autos\n",
      "Correctly Predicted: 7 Total: 10 Class: comp.sys.mac.hardware\n",
      "Correctly Predicted: 6 Total: 10 Class: alt.atheism\n",
      "Correctly Predicted: 6 Total: 10 Class: rec.sport.baseball\n",
      "Correctly Predicted: 4 Total: 10 Class: comp.os.ms-windows.misc\n",
      "Correctly Predicted: 9 Total: 10 Class: rec.sport.hockey\n",
      "Correctly Predicted: 8 Total: 10 Class: sci.crypt\n",
      "Correctly Predicted: 9 Total: 10 Class: sci.med\n",
      "Correctly Predicted: 9 Total: 10 Class: talk.politics.misc\n",
      "Correctly Predicted: 10 Total: 10 Class: rec.motorcycles\n",
      "Correctly Predicted: 5 Total: 10 Class: comp.windows.x\n",
      "Correctly Predicted: 7 Total: 10 Class: comp.graphics\n",
      "Correctly Predicted: 3 Total: 10 Class: comp.sys.ibm.pc.hardware\n",
      "Correctly Predicted: 4 Total: 10 Class: sci.electronics\n",
      "Correctly Predicted: 6 Total: 10 Class: talk.politics.guns\n",
      "Correctly Predicted: 8 Total: 10 Class: sci.space\n",
      "Correctly Predicted: 8 Total: 10 Class: soc.religion.christian\n",
      "Correctly Predicted: 5 Total: 10 Class: misc.forsale\n",
      "Correctly Predicted: 3 Total: 10 Class: talk.religion.misc\n",
      "Accuracy: 65.5 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_bulk(test_data, my_words_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
